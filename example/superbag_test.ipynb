{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opennre import encoder, model, framework\n",
    "import opennre\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '..'\n",
    "word2id = json.load(open(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_word2id.json')))\n",
    "word2vec = np.load(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_mat.npy'))\n",
    "rel2id = json.load(open('../benchmark/nyt10-aug/nyt10_rel2id.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-08 15:48:33,414 - root - INFO - Initializing word embedding with word2vec.\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder = opennre.encoder.PCNNEncoder(\n",
    "    token2id=word2id,\n",
    "    max_length=100,\n",
    "    word_size=50,\n",
    "    position_size=5,\n",
    "    hidden_size=230,\n",
    "    blank_padding=True,\n",
    "    kernel_size=3,\n",
    "    padding_size=1,\n",
    "    word2vec=word2vec,\n",
    "    dropout=0.5\n",
    ")\n",
    "train_loader = framework.BagRELoader(\n",
    "                '../benchmark/nyt10-aug/lt_train_augmented.txt',\n",
    "                rel2id,\n",
    "                sentence_encoder.tokenize,\n",
    "                160,\n",
    "                True,\n",
    "                bag_size=0,\n",
    "                entpair_as_bag=False)\n",
    "bag_encoder = opennre.model.IntraBagAttention(sentence_encoder, len(rel2id), rel2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(True)\n",
    "    )\n",
    "def calc_init_centroid(bag_reps, num_sbags_width, num_sbags_height):\n",
    "    \n",
    "    centroids = nn.functional.adaptive_avg_pool2d(bag_reps, (num_sbags_height, num_sbags_width))\n",
    "    with torch.no_grad():\n",
    "        num_sbags = num_sbags_width * num_sbags_height\n",
    "        labels = torch.arange(num_sbags).reshape(1, 1, *centroids.shape[-2:]).type_as(centroids)\n",
    "        init_label_map = nn.functional.interpolate(labels, size=(height, width), mode=\"nearest\")\n",
    "        init_label_map = init_label_map.repeat(1, 1, 1, 1)\n",
    "\n",
    "    init_label_map = init_label_map.reshape(1, -1)\n",
    "    centroids = centroids.reshape(1, 1, -1)\n",
    "\n",
    "    return centroids, init_label_map\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_abs_indices(init_label_map, num_sbags_width):\n",
    "    b, n_pixel = init_label_map.shape\n",
    "    device = init_label_map.device\n",
    "    r = torch.arange(-1, 2.0, device=device)\n",
    "    relative_spix_indices = torch.cat([r - num_sbags_width, r, r + num_sbags_width], 0)\n",
    "\n",
    "    abs_pix_indices = torch.arange(n_pixel, device=device)[None, None].repeat(b, 9, 1).reshape(-1).long()\n",
    "    abs_spix_indices = (init_label_map[:, None] + relative_spix_indices[None, :, None]).reshape(-1).long()\n",
    "    abs_batch_indices = torch.arange(b, device=device)[:, None, None].repeat(1, 9, n_pixel).reshape(-1).long()\n",
    " \n",
    "    return torch.stack([abs_batch_indices, abs_spix_indices, abs_pix_indices], 0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_hard_abs_labels(affinity_matrix, init_label_map, num_sbags_width):\n",
    "    relative_label = affinity_matrix.max(1)[1]\n",
    "    r = torch.arange(-1, 2.0, device=affinity_matrix.device)\n",
    "    relative_spix_indices = torch.cat([r - num_sbags_width, r, r + num_sbags_width], 0)\n",
    "    label = init_label_map + relative_spix_indices[relative_label]\n",
    "    return label.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BagCluster(nn.Module):\n",
    "    \"\"\"\n",
    "    Instance attention for bag-level relation extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bag_encoder, num_class, rel2id, cluster_num, num_iter):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bag_encoder: encoder for bag\n",
    "            num_class: number of classes\n",
    "            id2rel: dictionary of id -> relation name mapping\n",
    "            cluster_num: number of cluster\n",
    "            num_iter: number of iteration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bag_encoder = bag_encoder\n",
    "        self.num_class = num_class\n",
    "        self.cluster_num = cluster_num\n",
    "        self.num_iter = num_iter\n",
    "        self.feature_dim = 128\n",
    "        self.fc = nn.Linear(self.bag_encoder.sentence_encoder.hidden_size, num_class)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.rel2id = rel2id\n",
    "        self.id2rel = {}\n",
    "        self.drop = nn.Dropout()\n",
    "        for rel, id in rel2id.items():\n",
    "            self.id2rel[id] = rel\n",
    "            \n",
    "        self.scale1 = nn.Sequential(\n",
    "            conv_bn_relu(1, 64),\n",
    "            conv_bn_relu(64, 64)\n",
    "        )\n",
    "        self.scale2 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, 2, padding=1),\n",
    "            conv_bn_relu(64, 64),\n",
    "            conv_bn_relu(64, 64)\n",
    "        )\n",
    "        self.scale3 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, 2, padding=1),\n",
    "            conv_bn_relu(64, 64),\n",
    "            conv_bn_relu(64, 64)\n",
    "        )\n",
    "\n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(64*3+5, self.feature_dim-5, 3, padding=1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, labels, bag_reps, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bag_reps: (B, 3H) bag representations of a batch\n",
    "            num_sbags: (int) A number of superbags \n",
    "        Return:\n",
    "            cluster, (C, , 3H) C is the number of cluster, the cluster of bag.\n",
    "        \"\"\"\n",
    "        height,width = bag_reps.shape[-2:]\n",
    "        num_sbags_width = int(math.sqrt(num_sbags*width / height))\n",
    "        num_sbags_height = int(math.sqrt(num_sbags * height / width))\n",
    "\n",
    "        spixel_feature, init_label_map = calc_init_centroid(bag_feature, num_sbags_width, num_sbags_height)\n",
    "        abs_indices = get_abs_indices(init_label_map, num_sbags_width)\n",
    "        \n",
    "        bag_feature = bag_reps.reshape(*bag_reps[-2:], -1)\n",
    "        permuted_bag_feature = bag_feature.permute(0, 2, 1).contiguous()\n",
    "        \n",
    "        return bag_feature\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def convert_index(bag_num, label, sbag_num, is_random=True):\n",
    "    \"\"\"\n",
    "    初始化超包和包的对应索引\n",
    "    Args:\n",
    "    bag_num: 包的数量\n",
    "    label: 包对应的标签\n",
    "    sbag_num: 超包的数量\n",
    "    is_random: 是否随机初始化，否则为使用标签初始化。\n",
    "    return：\n",
    "    cor_map: b->sb的关系，包和超包的关联数组，索引i的值为j，表示第i个包属于第j个超包\n",
    "    sbag_map: sb->b的关系，(sbag_num, )\n",
    "    \"\"\"\n",
    "    cor_map = [0] * bag_num\n",
    "    if is_random:\n",
    "        for i in range(bag_num):\n",
    "            if i < sbag_num:\n",
    "                cor_map[i] = i\n",
    "            else:\n",
    "                cor_map[i] = math.floor(random.random() * sbag_num)\n",
    "    else:\n",
    "        visited_label = []\n",
    "        for idx, l in enumerate(label):\n",
    "            if l not in visited_label:\n",
    "                visited_label.append(l)\n",
    "            sbag_index = visited_label.index(l)\n",
    "            cor_map[idx] =sbag_index\n",
    "    sbag_map = [[] for _ in range(sbag_num)]\n",
    "    for idx, sbag_index in enumerate(cor_map):\n",
    "        sbag_map[sbag_index].append(idx)\n",
    "    return cor_map, sbag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SbagFeature(bag_feat, label, num_sbag):\n",
    "    \"\"\"\n",
    "    通过平均值获取超包的特征\n",
    "    Args: \n",
    "    bag_feat: 包特征(B,3H)\n",
    "    label: 包对应的label\n",
    "    num_sbag: 超包的个数\n",
    "    return \n",
    "    超包的特征\n",
    "    \"\"\"\n",
    "    bag_num, hidden_size = bag_feat.shape\n",
    "    cor_map, sbag_map = convert_index(bag_num, label, num_sbag, is_random=True)\n",
    "    ave_feat = []\n",
    "    for sbags_idx in sbag_map:\n",
    "        sbags_feat = []\n",
    "        for idx in sbags_idx:\n",
    "            try:\n",
    "                sbags_feat.append(bag_feat[idx])\n",
    "            except:\n",
    "                print(idx)\n",
    "        sbags_feat = torch.stack(sbags_feat)\n",
    "#         print(sbags_feat.size(), sbags_feat.shape)\n",
    "        ave_feat.append(torch.mul(sbags_feat.sum(0), 1/len(sbags_idx)))\n",
    "    return torch.stack(ave_feat), cor_map, sbag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Passoc(bag_feat, sbag_feat, sb2b_index, scale_value=1):\n",
    "    '''\n",
    "    calculate the distance between bag with each superbag. each iteration spixel_init is fixed,\n",
    "    only change the feature and association.\n",
    "    :param bag_feat: (B,3H)\n",
    "    :param sbag_feat: (D, 3H) D is the number of surpixels\n",
    "    :param p2sp_index_: (D)\n",
    "    :param scale_value:\n",
    "    :return:\n",
    "    distance (B*D*3H)\n",
    "    '''\n",
    "    b, h = bag_feat.shape\n",
    "    sb_num = len(sb2b_index)\n",
    "#     if len(p2sp_index_.shape) == 3:\n",
    "#         p2sp_index_ = torch.from_numpy(p2sp_index_).unsqueeze(0)\n",
    "#         invisible_ = torch.from_numpy(invisible_).unsqueeze(0)\n",
    "    bag_feat = bag_feat.repeat(1, sb_num).reshape(b, sb_num, h) # (B*D*3H)\n",
    "    sbag_feat = sbag_feat.repeat(b, 1).reshape(b, -1, h ) #(B*D*3H)\n",
    "\n",
    "    distance = torch.pow(sbag_feat - bag_feat, 2.0)  # 9*B*C*H*W  (occupy storage 440M)\n",
    "    distance = distance * scale_value  # B*D*3H\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_assignments(sbag_feature, bag_rep, sb2b_index):\n",
    "\n",
    "    pixel_spixel_neg_dist = Passoc(bag_rep, sbag_feature, sb2b_index)\n",
    "    pixel_spixel_assoc = (pixel_spixel_neg_dist - pixel_spixel_neg_dist.max(1, keepdim=True)[0]).exp()\n",
    "    pixel_spixel_assoc = pixel_spixel_assoc / (pixel_spixel_assoc.sum(1, keepdim=True))\n",
    "    \n",
    "    return pixel_spixel_assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpixelFeature2(bag_feature, weight, num_spixels):\n",
    "    '''\n",
    "    calculate spixel feature according to the similarity matrix between pixel and spixel\n",
    "    :param bag_feature: B*3H\n",
    "    :param weight:  B*D*3H\n",
    "    :return:B*3H\n",
    "    '''\n",
    "    b, h = bag_feature.shape\n",
    "    \n",
    "    feat = bag_feature.reshape(b, 1, h) # B*1*3H\n",
    "    \n",
    "    s_feat = feat * weight  #B*D*3H\n",
    "    \n",
    "#     s_feat = s_feat.reshape(b, 1, num_spixels, -1)  #B*D*(n/D)\n",
    "#     weight = weight.reshape(b, 1, num_spixels, -1)  #B*D*(n/D)\n",
    "    \n",
    "    weight = weight.sum(0)  #D*H\n",
    "    s_feat = s_feat.sum(0)  #D*H\n",
    "\n",
    "    S_feat = s_feat / (weight + 1e-5)\n",
    "    S_feat = S_feat * (weight > 0.001).float()\n",
    "    return S_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_iter(sbag_feature, bag_rep, sb2b_index):\n",
    "\n",
    "    # Compute pixel-superpixel assignments\n",
    "    # t3 = time.time()\n",
    "    # print(f't2-t1:{t2-t1:.3f}, t3-t2:{t3-t2:.3f}')\n",
    "    pixel_assoc = compute_assignments(sbag_feature, bag_rep, sb2b_index)\n",
    "    sbag_feat = SpixelFeature2(bag_rep, pixel_assoc, len(sb2b_index))\n",
    "    return sbag_feat, pixel_assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码，用于计算紧致度损失\n",
    "def decode_features(pixel_spixel_assoc, spixel_feat):\n",
    "    \"\"\"\n",
    "    :param pixel_spixel_assoc: B*D*3H the distance of each bag and each sbag\n",
    "    :param spixel_feat: B*D*3H sbag feature\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    b, d, h, = pixel_spixel_assoc.shape\n",
    "    recon_feat = spixel_feat.sum(1) + 1e-10  # B*3H\n",
    "\n",
    "    # norm\n",
    "    try:\n",
    "        assert recon_feat.min() >= 0., 'fails'\n",
    "    except:\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "    #\n",
    "    print(recon_feat.shape)\n",
    "    recon_feat = recon_feat / recon_feat.sum(1, keepdim=True)\n",
    "\n",
    "\n",
    "    return recon_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label, num=50):\n",
    "    \n",
    "    label = torch.tensor(label)\n",
    "    problabel = np.zeros((1, num, label.shape[0])).astype(np.float32)\n",
    "\n",
    "    ct = 0\n",
    "    for t in np.unique(label).tolist():\n",
    "        if ct >= num:\n",
    "            print(np.unique(label).shape)\n",
    "            break\n",
    "            # raise IOError\n",
    "        else:\n",
    "            problabel[:, ct, :] = (label == t)\n",
    "        ct = ct + 1\n",
    "\n",
    "    label2 = np.squeeze(np.argmax(problabel, axis = 1))\n",
    "\n",
    "    return label2, problabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_bag_rep(bag_rep, sbag_feat, b2sb_index):\n",
    "    res = []\n",
    "    for (idx,sb_idx) in enumerate(b2sb_index):\n",
    "        res.append((bag_rep[idx] + sbag_feat[sb_idx]) / 2.0)\n",
    "    return torch.stack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8491501"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = sentence_encoder.tokenizer\n",
    "file = open('./id2voc.txt', 'w+')\n",
    "file.write(json.dumps(tokenizer.inv_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23814,\n",
       " 23815,\n",
       " 23816,\n",
       " 23817,\n",
       " 23818,\n",
       " 23819,\n",
       " 23820,\n",
       " 69692,\n",
       " 69693,\n",
       " 69694,\n",
       " 69695,\n",
       " 69696,\n",
       " 69697,\n",
       " 69698]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.bag_scope[1225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.name2id[('m.02b3v0', 'm.013hxv', '/people/person/place_of_birth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('m.053x3n', 'm.0fnb4', '/people/person/place_of_birth') ['these', 'include', \"''\", 'the', 'best', 'poems', 'of', 'shamsur', 'rahman', ',', \"''\", 'published', 'last', 'year', 'in', 'new', 'delhi', ';', 'and', \"''\", 'the', 'devotee', ',', 'the', 'combatant', ':', 'choose', 'poems', 'of', 'shamsur', 'rahman', ',', \"''\", 'published', 'in', '2000', 'in', 'dhaka', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# bag_cluster = BagCluster(bag_encoder, len(rel2id), rel2id, 100, 100)\n",
    "num_iter = 50\n",
    "train = True\n",
    "for iter, data in enumerate(train_loader):\n",
    "#     if torch.cuda.is_available():\n",
    "#         for i in range(len(data)):\n",
    "#             try:\n",
    "#                 data[i] = data[i].cuda()\n",
    "#             except:\n",
    "#                 pass\n",
    "    label = data[0]\n",
    "    bag_name = data[1]\n",
    "    scope = data[2]\n",
    "    tokens = data[3]\n",
    "    args = data[3:]\n",
    "    print(bag_name[0], sentence_encoder.tokenizer.convert_ids_to_tokens(tokens[0][2].numpy().tolist()))\n",
    "#     sentence_encoder.tokenizer.convert_ids_to_tokens(tokens[0][0])\n",
    "#     bag_rep = bag_encoder(label, scope, *args, bag_size=0)\n",
    "#     sbag_feature, b2sb_index, sb2b_index = SbagFeature(bag_rep, label, 20)\n",
    "    break\n",
    "#     for i in range(num_iter):\n",
    "#         sbag_feature, _ = exec_iter(sbag_feature, bag_rep, sb2b_index)\n",
    "        \n",
    "#     final_bag_assoc = compute_assignments(sbag_feature, bag_rep, sb2b_index)\n",
    "#     if train:\n",
    "#         new_sbag_feat = SpixelFeature2(bag_rep, final_bag_assoc, len(sb2b_index))\n",
    "#         print(new_sbag_feat.shape)\n",
    "# #         new_spix_indices = compute_final_spixel_labels(final_bag_assoc, p2sp_index)\n",
    "# #         recon_feat2 = Semar(new_spixel_feat, new_spix_indices)\n",
    "# #         problabel = convert_label(label)\n",
    "# #         print(problabel[0], problabel[0].shape)\n",
    "# #         spixel_label = SpixelFeature2(problabel, final_bag_assoc, len(sb2b_index))\n",
    "#         final_bag_feat = compute_final_bag_rep(bag_rep, new_sbag_feat, b2sb_index)\n",
    "    \n",
    "#         print(final_bag_feat, final_bag_feat.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_index(bag_num, labels, sbag_num, is_random=True):\n",
    "    \"\"\"\n",
    "    初始化超包和包的对应索引\n",
    "    Args:\n",
    "    bag_num: 包的数量\n",
    "    label: 包对应的标签\n",
    "    sbag_num: 超包的数量\n",
    "    is_random: 是否随机初始化，否则为使用标签初始化。\n",
    "    return：\n",
    "    cor_map: b->sb的关系，包和超包的关联数组，索引i的值为j，表示第i个包属于第j个超包\n",
    "    sbag_map: sb->b的关系，(sbag_num, )\n",
    "    cluster_labels: sb对应的label\n",
    "    \"\"\"\n",
    "    cor_map = [-1] * bag_num\n",
    "    if is_random:\n",
    "        cluster_labels = labels\n",
    "        for i in range(bag_num):\n",
    "            if i < sbag_num:\n",
    "                cor_map[i] = i\n",
    "            else:\n",
    "                cor_map[i] = math.floor(random.random() * sbag_num)\n",
    "    else:\n",
    "        # 先将不重合的超包标签定下来\n",
    "        superbag_labels = list(set(labels))\n",
    "        for (idx, label) in enumerate(superbag_labels):\n",
    "            idx = labels.index(label)\n",
    "            cor_map[idx] = idx\n",
    "        # 当前超包的数量小于需要的数量，加入重合的超包标签\n",
    "        iter_num = sbag_num - len(superbag_labels)\n",
    "        for _ in range(iter_num):\n",
    "            # 将第一个出现的未标记的包的标签标记为新的超包标签\n",
    "            empty_idx = cor_map.index(-1)\n",
    "            cor_map[empty_idx] = len(superbag_labels)\n",
    "            superbag_labels.append(labels[empty_idx])\n",
    "        for (idx, label) in enumerate(cor_map):\n",
    "            if label < 0:\n",
    "                cur_label = labels[idx]\n",
    "                superbag_idxs = [idx for (idx, label) in enumerate(superbag_labels) if label == cur_label]\n",
    "                selected = random.choice(superbag_idxs)\n",
    "                cor_map[idx] = selected\n",
    "    sbag_map = [[] for _ in range(sbag_num)]\n",
    "    print(cor_map)\n",
    "    for idx, sbag_index in enumerate(cor_map):\n",
    "        sbag_map[sbag_index].append(idx)\n",
    "    return cor_map, sbag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[0,1,3],[3,3,3]])\n",
    "b = torch.Tensor([[1,4,3],[2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  9.])\n",
      "tensor([5., 2.])\n"
     ]
    }
   ],
   "source": [
    "assignments = []\n",
    "for i in a:\n",
    "    distances = torch.pow(i - b, 2).sum(1)\n",
    "    print(distances)\n",
    "    assignments.append(distances.argmin().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 4., 4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[a].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 0, 3, 5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bag_encoder.id2rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opennre",
   "language": "python",
   "name": "opennre"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
